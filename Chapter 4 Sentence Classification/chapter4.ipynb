{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <span style=\"color:yellow\">**Handling variable-length input with Recurrent Neural Network (RNN)**</span>.\n",
    "* <span style=\"color:green\">**Working with RNNs and their variants (LSTMs and GRUs)**</span>.\n",
    "* <span style=\"color:pink\">**Using common evaluation metrics for classification problems**</span>.\n",
    "* <span style=\"color:red\">**Developing and configuring a training pipeline using AllenNLP**</span>.\n",
    "* <span style=\"color:blue\">**Building a language detector as a sentence classification task**</span>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Recurrent neural networks (RNNs)\n",
    "* The first step in sentence classification is to represent variable-length sentences using neural networks (RNNs).\n",
    "* In this section, I'm going to present the concept of recurrent neural networks, one of the most important concepts in deep NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1 Handling variable-length input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neural networks can handle only numbers and arithmetic operations. => That was why we needed to convert words and documents to numbers through embeddings.\n",
    "* One idea is to first convert the input to embeddings, then average them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![alt text for screen readers](figure1.PNG \"Text to show on mouseover\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This method is quite simple and is actually used in many NLP applications.\n",
    "* **BUT, it has one critical issue, which is that it cannot take word order into account.**<br>\n",
    "**=> Solution: Use Recurrent neural networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2 RNN abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Reading process:** <br>\n",
    "1. Read a word. <br>\n",
    "2. Base on what has been read so far, figure out what the word means. <br>\n",
    "3. Update the mental state. <br>\n",
    "4. Move on to the next word. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![alt text for screen readers](figure2.PNG \"Text to show on mouseover\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3 Simple RNNs and nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Long short-term memory units (LSTMs) and gated recurrent units (GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The simple RNNs are rarely used in real-world NLP applications due to one problem called the **vanishing gradients problem**.\n",
    "* In this section, I'll show the issue associated with simple RNNs and how morepopular RNN architectures, namely LSTMs and GRUs, solve this particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.1 Vanishing gradients problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNNs trained with **back propagation algorithm**.\n",
    "* **Vanishing gradients problem**: the message needs to pass through many layers, it becomes so weak an obscure (or so strong and skewed because of some misunderstanding) that the inner functions have a difficult time figuring out what they did wrong. <br>\n",
    "**=> Because of the vanishing gradients problem, simple RNNs are difficult to train and rarely used in practice nowadays.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2 Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead of passing the information through an activation function every time and changing its shape completely, how about adding and subtracting information relevant to the part of sentence being processed at each step?\n",
    "* Long short-term memory units (LSTMs) are a type of RNN cell that is proposed based on this insight.\n",
    "* Instead of passing around states, LSTM cells share a \"memory\" that each cell can remove old information from and/or add new information to, some-thing like an assembly line in manufacturing factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lstm(state, word):\n",
    "    cell_state, hidden_state = state\n",
    "    cell_state *= forget(hidden_state, word)\n",
    "    cell_state += add(hidden_state, word)\n",
    "\n",
    "    hidden_state = update_hidden(hidden_state, cell_state, word)\n",
    "\n",
    "    return (cell_state, hidden_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The LSTM state comprise two halves - the cell state (the \"memory\" part) and the hidden state (the \"mental representation\" part).\n",
    "* The function forget() returns a value between 0 and 1, so multiplying by this number mean erasing old memory from cell_state. How much to erase is determined from hidden_state and word (input). Controlling the flow of information by multiplying by a value between 0 and 1 is called **gating**. LSTMs are the first RNN architecture that uses this gating mechanism.\n",
    "* The function add() return a new value added to the memory. The value again is determined from hidden_state and word.\n",
    "* Finally, hidden_state is updated using a function, whose value is computed from the previous hidden state, the updated memory, and the input word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![alt text for screen readers](figure8.PNG \"Text to show on mouseover\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.3 Gated recurrent units (GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gated Recurrent Units (GRUs), uses the gating mechanism. The philosophy behind GRUs is similar to that of LSTMs.\n",
    "* BUT, GRUs use only one set of states instead of two halves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_gru(state, word):\n",
    "    new_state = update_hidden(state, word)\n",
    "\n",
    "    switch = get_switch(state, word)\n",
    "\n",
    "    state = switch * new_state + (1 - switch) * state\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead of erasing or updating the memory, GRUs use a switching mechanism.\n",
    "1. The Cell first computes the new state from the old state and the input. \n",
    "2. It then compute *switch*, a value between 0 and 1. The state is chosen between the new state and the old one based on the value of switch.\n",
    "3. The state is updated by: state = switch * new_state + (1 - switch) * old_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![alt text for screen readers](figure9.PNG \"Text to show on mouseover\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Accuracy, precision, recall, and F-measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Building AllenNLP training pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29842f096b039504e55cf96bcffd01d9cbf89740ad11d5616300fb38b587160c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
