{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your First NLP Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Building a sentiment analyzer using AllenNLP**\n",
    "* **Applying basic machine learning conceopts (datasets, classification, and regression)**\n",
    "* **Employing neural network concepts (word embeddings, recurrent neural networks, linear layers)**\n",
    "* **Training the model through reducing loss**\n",
    "* **Evaluating and deploying your model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Introducing Sentiment Analysis\n",
    "![alt text for screen readers](figure2.1.PNG \"Text to show on mouseover\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Working with NLP datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 What is a dataset?\n",
    "* A dataset simply means a collection of data.\n",
    "* In NLP, records in a dataset are usually some type of linguistic units, such as words,\n",
    "sentences, or documents. A dataset of natural language texts is called a corpus (plural:\n",
    "corpora).\n",
    "* A closely related term to a record is an instance. In machine learning, an instance is\n",
    "a basic unit for which the prediction is made.\n",
    "* A label is a piece of information attached to some linguistic unit in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure2.2.PNG\" alt=\"MarineGEO circle logo\" style=\"height: 400px; width:900px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Stanford Sentiment Treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Link: https://nlp.stanford.edu/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3 Train, Validation, and Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A train (or training) set is the main dataset used to train the NLP/ML models.\n",
    "* A validation set (also called a dev or development set) is used for model selection. The validation set is also used for tuning hyperparameters. \n",
    "* Finally, a test set is used to evaluate the model using a new, unseen set of data.\n",
    "\n",
    "<img src=\"figure2.3.PNG\" alt=\"MarineGEO circle logo\" style=\"height: 400px; width:700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.4 Loading SST datasets using AllenNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'realworldnlp'...\n",
      "fatal: '$GIT_DIR' too big\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/mhagiwara/realworldnlp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from allennlp.data.data_loaders import MultiProcessDataLoader\n",
    "from allennlp.data.samplers import BucketBatchSampler\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training import GradientDescentTrainer\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from allennlp_models.classification.dataset_readers.stanford_sentiment_tree_bank import \\\n",
    "    StanfordSentimentTreeBankDatasetReader\n",
    "\n",
    "from predictors import SentenceClassifierPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# path = \"C:\\\\Users\\\\longln3\\\\OneDrive - Hanoi University of Science and Technology\\\\Real-World-Natural-Language-Processing-Practical-applications-with-deep-learning-Masato-Hagiwara-\\\\realworldnlp\"\n",
    "# sys.path.append(path)\n",
    "# from realworldnlp.realworldnlp import SentenceClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Using word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 What are word embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word embeddings are one of the most important concepts in modern NLP.\n",
    "* Technically, an embedding is a continuous vector representation of something that is usually discrete.\n",
    "* A word embedding is acontinuous vector representation of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2 Using word embeddings for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = StanfordSentimentTreeBankDatasetReader()\n",
    "train_path = './trees/train.txt'\n",
    "dev_path = './trees/dev.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44125ef95153468799adedbf7f6e4929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0ae93ff1214b2190505b7768829f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\n",
    "train_data_loader = MultiProcessDataLoader(reader, train_path, batch_sampler=sampler)\n",
    "dev_data_loader = MultiProcessDataLoader(reader, dev_path, batch_sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb0f143625f4ea88fe4fc06e6206109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can optionally specify the minimum count of tokens/labels.\n",
    "# `min_count={'tokens':3}` here means that any tokens that appear less than three times\n",
    "# will be ignored and not included in the vocabulary.\n",
    "vocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(), dev_data_loader.iter_instances()),\n",
    "                                  min_count={'tokens': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader.index_with(vocab)\n",
    "dev_data_loader.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
    "# not for labels, which are used as-is as the \"answer\" of the sentence classification\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model in AllenNLP represents a model that is trained.\n",
    "class LstmClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 embedder: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary,\n",
    "                 positive_label: str = '4') -> None:\n",
    "        super().__init__(vocab)\n",
    "        # We need the embeddings to convert word IDs to their vector representations\n",
    "        self.embedder = embedder\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # After converting a sequence of vectors to a single vector, we feed it into\n",
    "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
    "        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                      out_features=vocab.get_vocab_size('labels'))\n",
    "\n",
    "        # Monitor the metrics - we use accuracy, as well as prec, rec, f1 for 4 (very positive)\n",
    "        positive_index = vocab.get_token_index(positive_label, namespace='labels')\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        self.f1_measure = F1Measure(positive_index)\n",
    "\n",
    "        # We use the cross entropy loss because this is a classification task.\n",
    "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
    "        # which makes it unnecessary to add a separate softmax layer.\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Instances are fed to forward after batching.\n",
    "    # Fields are passed through arguments with the same name.\n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
    "        # shorter sequences get padded with zeros to make them equal length.\n",
    "        # Masking is the process to ignore extra zeros added by padding\n",
    "        mask = get_text_field_mask(tokens)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = self.embedder(tokens)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.linear(encoder_out)\n",
    "\n",
    "        # In AllenNLP, the output of forward() is a dictionary.\n",
    "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            self.f1_measure(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {'accuracy': self.accuracy.get_metric(reset),\n",
    "                **self.f1_measure.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
    "# (usually a sequence of embedded word vectors), processes it, and returns a single\n",
    "# vector. Oftentimes this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
    "# AllenNLP also supports CNNs and other simple architectures (for example,\n",
    "# just averaging over the input vectors).\n",
    "encoder = PytorchSeq2VecWrapper(\n",
    "    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LstmClassifier(word_embeddings, encoder, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8b4f25dfc2484cbb1fb664df5d8a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a698ca5d93a7420cae6fa05f774724bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c353d2286ad447e2900db89de4c63284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9518978c8464bd682c976f32d79bed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b788296d69c243b4985f48beb34349b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f07d7f624ac4143878b6b4597208d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47aeed6bf0040daa602a6c0cbc6c64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a063e27315e243fc8038d977d7174f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3205423511ac49299c2acd4fad187454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342ea5924c0c4cd2b92dbd5a3123bd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ac73692af2494e919487ffc9a394d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdb8fec376a4171a5eb6029df6bd618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb530f57cce41e984692250cefe6e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2fdfccd844448bb6f1b1ca8ee755b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af869b9c40404c27ad7a8e434c6b5f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365e21a6afe4482883bdd2281f30e8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf64a24759724372a75a2f62adede398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e3289cd9d84b8698f0c1b3bf16069e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038c7958cedd46f29d1e25ba1621bb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fce7eeb43c4a1088cb86afceeef085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6440899bd54ba591955674c4e69c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed889f9ddaf48ddb2119a3ccbb0e871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79210b934ae4c12b81af0f0354a62ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a18e8542625490f8a9097a209517e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14961154e7f84fde82d9013dffb4c0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38697b2bed44ffeaadad5ca4573e981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da6cc5f40094b5397dab234b74689fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec68b3d8b0764be9a15197dfe41b3017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afc0ff73a72440b802f524b5bc795ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7091b9961f6457fa38c43a275728800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90eb7849964c420fa1a593e6057da0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f64d396cb3841bd9239e1f857322240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 5,\n",
       " 'peak_worker_0_memory_MB': 0,\n",
       " 'training_duration': '0:06:30.002704',\n",
       " 'epoch': 15,\n",
       " 'training_accuracy': 0.7813670411985019,\n",
       " 'training_precision': 0.8778452277183533,\n",
       " 'training_recall': 0.8982919454574585,\n",
       " 'training_f1': 0.8879508972167969,\n",
       " 'training_loss': 0.5738753581538183,\n",
       " 'training_worker_0_memory_MB': 0.0,\n",
       " 'validation_accuracy': 0.3614895549500454,\n",
       " 'validation_precision': 0.3563218414783478,\n",
       " 'validation_recall': 0.3757575750350952,\n",
       " 'validation_f1': 0.3657817244529724,\n",
       " 'validation_loss': 2.3366266523088726,\n",
       " 'best_validation_accuracy': 0.335149863760218,\n",
       " 'best_validation_precision': 0.30249109864234924,\n",
       " 'best_validation_recall': 0.5151515007019043,\n",
       " 'best_validation_f1': 0.38116592168807983,\n",
       " 'best_validation_loss': 1.5144107137407576}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = GradientDescentTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_data_loader,\n",
    "    validation_data_loader=dev_data_loader,\n",
    "    patience=10,\n",
    "    num_epochs=20,\n",
    "    cuda_device=-1)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spacy models 'en_core_web_sm' not found.  Downloading and installing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
    "logits = predictor.predict('This is the best movie ever!')['logits']\n",
    "label_id = np.argmax(logits)\n",
    "\n",
    "print(model.vocab.get_token_from_index(label_id, 'labels'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29842f096b039504e55cf96bcffd01d9cbf89740ad11d5616300fb38b587160c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
